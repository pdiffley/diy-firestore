# DIY Firestore

## First step: The database

Before we implement the more complex features of our database we need to set up
an underlying datastore. We are going to put all of our data into a SQL
database, specifically Postgres version 15. It is a little unintuitive that we
are using a SQL to support our schemaless database, but SQLâ€™s gold standard ACID
transactions will make it easy to implement the broad feature set we need to
support.

### Storing our data

The first thing we will do is create a table to hold our our schemaless
documents,

<CH.Code>

```sql create_documents.sql
CREATE TABLE documents (
  collection_parent_path      TEXT,
  collection_id               TEXT,
  document_id                 TEXT,
  document_data               BYTEA,
  update_id                   TEXT,
  PRIMARY KEY (collection_parent_path, collection_id, document_id)
);
```

</CH.Code>

Each row in the `documents` table holds the unique identifiers for a document,
`collection_parent_path`, `collection_id`, and `document_id`, as well as a blob
holding the document data itself. We also included a column `update_id` which
we'll make use of later.

So that we can retrieve documents from the table based on their identifiers, we
will create the index

```sql
CREATE INDEX collection_id_collection_parent_path_idx
ON documents(collection_id, collection_parent_path, document_id, update_id);
```

### Binary Document

To store our documents in the document_data column, we need a way to get a
binary representation of our document. Protobuf's is a convenient way to do
this. We will define a document proto to have a document id and a map of fields.

```proto
syntax = "proto3";
package protos.documents;

message DocumentId {
    string collection_parent_path = 1;
    string collection_id = 2;
    string document_id = 3;
}

message Document {
  DocumentId id = 1;
  map<string, FieldValue> fields = 2;
  optional string update_id = 3;
}
```

Protobuf messages have a schema though. To allow for multiple datatypes to be
stored in our document, we will define a type FieldValue that can hold all of
the possible value our documents support. Protobuf's "oneof" feature
conveniently allows us to indicate that any given FieldValue will only contain
one of these types.

```proto
message FieldValue {
  oneof value {
    Unit null_value = 1;
    bool boolean_value = 2;
    int64 integer_value = 3;
    double double_value = 4;
    Timestamp timestamp_value = 5;
    string string_value = 6;
    bytes bytes_value = 7;
    string reference_value = 8;
  }
}

enum Unit {
  NotNull = 0;
}

message Timestamp {
  int64 nanos = 1;
  int64 seconds = 2;
}
```

With this protobuf representation, we can serialize our documents to a binary
format and store them in our document_data column. Note that using protobuf is
also conventient because we can also use the same binary representation to pass
documents between our client libraries and backend.

### Review

We have just made two fundamental design choices that will direct how we
implement our database.

First, we chose to use a SQL database as our underlying datastore. SQL's acid
transactions make it easy for us to support all of Firestore's features without
worrying about our database getting into in an inconsistent state. However,
using a SQL database places some limits on scalability which is one of
Firestore's selling points. Using a traditional SQL database, we are limited to
vertically scaling our write capacity. Vertical scaling is acceptable for use
cases, but the idea that you might have to fully rearchitect your system at a
future date because your database can't scale to your needs can feel troubling
when you are starting a project (even if you know you are optimizing
prematurely). That said, if we needed horizontal scalability, we could use a
horizontally scaleable SQL database like Spanner instead (which now has a
Postgres interface).

We also chose to represent documents in our database as serialized Protobufs.
Protobuf is a widely used standard with cross language support and good
performance, so it is convenient for our use case, but there are other options
we could have chosen.

One notable alternative, would have been to use JSON. This is particularly
tempting as Postgres allows you to store JSON documents in a binary JSONB column
and perform queries over the fields in those JSON documents. JSON falls short
for us in a couple ways though. The biggest issue is that JSON does not
distinguish between integer and floating point numeric types ("1" and "1.0" are
identical in JSON). We need our documents to distinguish between 64 bit numbers
of both integer and floating point types. If we naively used JSON to hold our
numeric types, we would lose the type information needed to translate the JSON
numbers back to the appropriate type in our client applications. Keeping track
of the type information manually would be a pain and is just not worth the
effort.

Postgres's support for querying JSON is not very helpful for our use cases
either. To query a JSONB column efficiently, you need to create an index
specifying which JSON fields you want to query in advance. We need to support
queries on any field in any document at any time, so that won't work for us.

### Next Up

Now that we know how we are storing our data, let's create functions to write
documents to the database!
