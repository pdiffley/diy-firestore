---
title: The Database
index: 3
---
# The Database

Before we implement the more complex features of our database we need to set up
an underlying datastore. We are going to put all of our data into a SQL
database, specifically Postgres version 15. It is a little counterintuitive that we
are using SQL to support our schemaless database, but we will see over the course
of the series how SQL's gold standard ACID transactions make it easy to implement 
the broad feature set we need to support.

## Storing our data

The first thing we will do is create a table to hold our our schemaless
documents,

```sql
CREATE TABLE documents (
  collection_parent_path      TEXT,
  collection_id               TEXT,
  document_id                 TEXT,
  document_data               BYTEA,
  update_id                   TEXT,
  PRIMARY KEY (collection_parent_path, collection_id, document_id)
);
```

Each row in the `documents` table holds the unique identifiers for a document &mdash;
`collection_parent_path`, `collection_id`, and `document_id` &mdash; as well as a blob
holding the document data itself. We also included a column `update_id` which we will
change to a new unique value anytime a document is updated. We will make use of this later.

So that we can retrieve documents from the table based on their identifiers, we
will create an index

```sql
CREATE INDEX collection_id_collection_parent_path_idx
ON documents(collection_id, collection_parent_path, document_id, update_id);
```

## Binary document representation

To store our documents in the document_data column, we need a way to get a
binary representation of our document. [Google's Protocol Buffers](https://protobuf.dev/)
("protobuf") provide a convenient way to do this. We will define a `Document`
protobuf message to have a document id and a map of fields.

```proto
syntax = "proto3";
package protos.documents;

message DocumentId {
    string collection_parent_path = 1;
    string collection_id = 2;
    string document_id = 3;
}

message Document {
  DocumentId id = 1;
  map<string, FieldValue> fields = 2;
  optional string update_id = 3;
}
```

Again, like Postgres, Protobuf messages have a schema. To allow for multiple data types to be
stored in our document, we define a type `FieldValue` that can hold all of
the possible values our documents support. Protobuf's `oneof` feature
conveniently allows us to indicate that any given FieldValue will only contain
one of these types.

```proto
message FieldValue {
  oneof value {
    Unit null_value = 1;
    bool boolean_value = 2;
    int64 integer_value = 3;
    double double_value = 4;
    Timestamp timestamp_value = 5;
    string string_value = 6;
    bytes bytes_value = 7;
    string reference_value = 8;
  }
}

enum Unit {
  NotNull = 0;
}

message Timestamp {
  int64 nanos = 1;
  int64 seconds = 2;
}
```

With this protobuf representation, we can serialize our documents to a binary
format and store them in our document_data column. 

## Review

We have just made two fundamental design choices that will direct how we
implement our database.

First, we chose to use a SQL database as our underlying datastore. SQL's acid
transactions make it easy for us to support all of Firestore's features without
worrying about our database getting into in an inconsistent state. However,
using a SQL database places some limits on scalability, one of
Firestore's selling points. Using a traditional SQL database, we are limited to
vertically scaling our write capacity. Vertical scaling is acceptable for most use
cases, but the idea that you might have to fully rearchitect your system at a
future date because your database can't scale to your needs can be troubling
when you are starting a project (even if you know you are optimizing
prematurely). That said, if we needed horizontal scalability, we could upgrade to a
horizontally scaleable SQL database like [Spanner](https://cloud.google.com/spanner/).

We also chose to represent documents in our database as serialized protobuf messages.
Protobuf is a widely used standard with cross language support and good
performance, so it is convenient for our use case, but there are other options
we could have chosen.

One notable alternative would have been to use JSON. This is particularly
tempting as Postgres allows you to store JSON documents in a binary JSONB column
and perform queries over the fields in those JSON documents. JSON falls short
for us in a couple ways though. The biggest issue is that JSON does not
distinguish between integer and floating point numeric types (`1` and `1.0` are
identical in JSON). We need our documents to distinguish between 64 bit numbers
of both integer and floating point types. If we used JSON to hold our
numeric types in a naive manner, we would lose the type information needed to translate the JSON
numbers back to the appropriate type in our client applications. Keeping track
of the type information manually would be a pain and is just not worth the
effort.

Postgres's support for querying JSON is not very helpful for our use cases
either. To query a JSONB column efficiently, you need to create an index
specifying which JSON fields you want to query in advance. We need to support
queries on any field in any document at any time, so that won't work for us.

## Next up

Now that we know how we are storing our data, we will create functions to write
and read documents.
