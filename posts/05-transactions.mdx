---
title: Transactions
index: 5
---
### Transactions

Firestore allows us to perform optimistic client side transactions, ensuring
that a set of writes only go through as a block or not at all.

This isn't too difficult to support. The first thing to notice is that our
document proto includes the `update_id` for a document. This mean that when our
client reads a document it also receives the update id for that document. To
allow client side transactions we just need to provide an endpoint that lets a
client specify a set of writes to perform along with a list of documents and
that need to have the same update_id as when the client read them in.

We will create a struct to specify the details of a write operation

```rust
pub struct TransactionOperationValue {
  operation: TransactionOperation,
  document: Document,
  relevant_composite_groups: Vec<CompositeFieldGroup>,
}

pub enum TransactionOperation {
  Write,
  Delete,
}
```

Then we will provide a function that takes both a list of documents read in the
transaction and the list of write operations the transaction will perform.

```rust
pub fn commit_transaction(sql_transaction: &mut Transaction, user_id: &UserId, read_documents: &[Document], write_operations: &[TransactionOperationValue]) -> bool {
  for document in read_documents {
    if document_has_changed(sql_transaction, document) {
      return false;
    }
  }

  for operation in write_operations {
    match operation.operation {
      TransactionOperation::Write => write_document(sql_transaction, user_id, operation.document.clone(), &operation.relevant_composite_groups),
      TransactionOperation::Delete => {
        let collection_parent_path = operation.document.id.clone().unwrap().collection_parent_path;
        let collection_id = operation.document.id.clone().unwrap().collection_id;
        let document_id = operation.document.id.clone().unwrap().document_id;
        delete_document(sql_transaction, user_id, &collection_parent_path, &collection_id, &document_id, &operation.relevant_composite_groups)
      }
    }
  }

  true
}

fn document_has_changed(transaction: &mut Transaction, document: &Document) -> bool {
  let collection_parent_path = document.id.clone().unwrap().collection_parent_path;
  let collection_id = document.id.clone().unwrap().collection_id;
  let document_id = document.id.clone().unwrap().document_id;
  let update_id = document.update_id.clone();

  return if let Some(update_id) = update_id {
    transaction.query(
      "SELECT 1 FROM documents WHERE collection_parent_path=$1 and collection_id=$2 and document_id=$3 and update_id=$4",
      &[&collection_parent_path, &collection_id, &document_id, &update_id],
    ).unwrap().len() == 0
  } else {
    transaction.query(
      "SELECT 1 FROM documents WHERE collection_parent_path=$1 and collection_id=$2 and document_id=$3",
      &[&collection_parent_path, &collection_id, &document_id],
    ).unwrap().len() == 0
  };
}
```

If the `update_id` for one of the `read_documents` has changed, the transaction
fails. Otherwise, we apply the set of write operations. It will be the
responsibility of the client library to make keep track the documents read in a
transaction.

Providing an update_id for our documents, allows us identify when the document
retrieved by a client is out of date. The side affect of performing transactions
this way is that our writes do not get sent to the database until after we have
performed all of the reads in the transaction.

### Pessimistic Transactions

The transactions we implemented above are optimistic. When we attempt to perform the
tranaction's writes, we check whether any of the previously read documents have changed
and if any have, the transaction fails. We do not make any attempt to prevent the documents
we are reading from being changed while the transaction is running. This works fine for 
a lot of cases, but if you are editing documents that are likely to have a lot of write
contention, this approach is ineffective, and you can end up in a situation where
no writes are able to get through at all. Pessimistic transactions prevent this issue
by locking the documents that used in a transaction and making other transactions that want
to use those documents block until the locks are free.


Solution: 
first: could open long lived connection with a serverside transaction taking the reads and
writes as they are made. -> poor performance due to many network calls in the transaction

second: provide option to have functions that run on the database server and run the desired transactions


If you have many clients trying to read and write to the same document in a transaction
at the same time, all of the transactions will fail, then retry, then fail again and so on.
One workaround for this problem is to use an exponential backoff, where after each failed
attempt at running a transaction, a client will wait an exponentially increasing amount of 
time before retrying. Exponentially increasing the retry delay helps spread out the requests 
until they no longer conflict with each other. It also means you clients could end up waiting
a long time for their transaction to go through, so this solution is less than ideal.

Instead we would like our database to support locking pessimistic transactions. In this
situation, the first transaction will acquire locks for the documents it is reading and writing, 
then subsequent transactions will 

### Next up

Now we are getting into the more distinguishing features of firestore. Next
we'll provide support subscriptions to documents and collections.
